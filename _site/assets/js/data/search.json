[
  
  {
    "title": "Interview with Emir Hamurcu",
    "url": "/posts/interview-emir-hamurcu/",
    "categories": "Interviews, Robotics",
    "tags": "robotics, interview, biped, modular robotics framework",
    "date": "2023-10-21 00:00:00 +0100",
    





    
    "snippet": "Emir Hamurcu is a 17 year old maker from Turkey who has been working on a bipedal robot using the Modular Robotics Framework. He very kindly allowed me to interview him about his project and his jo...",
    "content": "Emir Hamurcu is a 17 year old maker from Turkey who has been working on a bipedal robot using the Modular Robotics Framework. He very kindly allowed me to interview him about his project and his journey into robotics.Can you introduce yourself and share a bit about your background and how you got into robotics and making?Hello, I’m Emir Hamurcu. I’m 17 years old and I live in the Defne district of Hatay, Turkey. I’m a maker who loves to create creative projects and a young programmer. My interests have been oscillating between mechatronics and software since my childhood. I learned to speak at the age of 6 and started disassembling electronic devices at the age of 3.5-4. My curiosity about how machines work and my desire to create new things grew over time. In my younger years, I was often sought after by people in my community to fix their devices. However, I never asked for money for this service because every time I opened a device, I was learning new things about the mechanisms or circuits inside. These experiences allowed me to accumulate valuable knowledge and expand my expertise.My interest in the world of electronics gradually shifted towards the world of software in the following years. Towards the end of 6th grade, I decided to take my first steps in programming. I started with C++, and learning to program excited me. However, my curiosity about mechanical engineering continued at the same time. While contemplating how to combine these two interests, I encountered Arduino. Arduino was a perfect platform for developing both software and mechanical projects. After learning Arduino, I delved into programming languages such as Java, Python, C#, and developed a passion for learning these languages.In the 10th grade, I had an opportunity to develop a meaningful project for competitions. I created a bionic arm for disabled individuals. This arm could be controlled using a brain-computer interface and included a Raspberry Pi-based tablet. I developed software to assist disabled individuals in their daily lives and integrated these programs into my own operating system, “SentryOS.” I successfully passed the initial screening with this project, but I narrowly missed making it to the final round. On the competition day, I attended as an audience member and was greatly impressed, especially by the project that won first place in the category for disabled individuals. This project involved a simple parking sensor created using an HC-SR04 sensor and a buzzer. This experience made me realize the limited emphasis on science in our country. As a result, I decided to go to a country where I could find more support and opportunities, and I knew that I needed to work hard for it.After long contemplation, the thought of finding a way to go abroad became a recurring loop in my mind, which motivated me to work harder.During this process, I used to describe myself as a lazy person, and I also had issues such as social anxiety and overthinking, which made it difficult for me to communicate with others at school. However, these challenges directed me towards working more and improving myself. For two years, I immersed myself in learning various programming languages and programs, including “html, css, js, react, ts, bootstrap, nodejs, php, sql, TF, opencv” while also learning languages like Java, Python, and C#. I had just completed the first semester of the 11th grade, and learning these languages made me very happy. I believed that I could stand out in job applications after graduating.I always had a dream of creating an artificial intelligence robot project, but I thought I didn’t have enough time to start and my morale was low. However, I never forgot about this project and promised myself to bring it to life one day. After the first semester of the 11th grade ended, on the first day of the second semester, February 6th at 4:17 a.m., a series of earthquakes occurred in our country, affecting 11 provinces, including the one I lived in. This disaster resulted in schools being closed for a long time, as well as issues like electricity, water, and internet disruptions. There were certainly many downsides brought by the earthquake. However, a lot changed for me. After the earthquake, my life resembled the early days of my life. During this period, I made new friends, began to overcome my anxiety to some extent, and also came across the idea of developing a bionic robot with artificial intelligence, thanks to Dan Nicholson. His framework fascinated me. Since schools were closed for an extended period, I needed a different activity, and this framework caught my attention because of its flexibility and modularity. To be honest, I had no idea how to start, how to progress, or where to find materials, but I realized that it was very helpful for progress. When I couldn’t find the materials, I started producing my own parts - such as solenoids, screws, nuts, spacers - using 3D printing. At the same time, I learned advanced modeling with Fusion 360 while doing this, making my robot more modular and successfully building my own robot with additional features using the framework. Such an accomplishment provides incredible self-confidence because you are doing a project you first encountered on your own and adding different additional features yourself, which ignites creativity and curiosity in me.Can you introduce your project and what inspired you to start working on it?My project focuses on developing an open-source robotics platform. This platform will be flexible and customizable in terms of both software and hardware, making it a useful tool for those working on robotics projects. The platform will be easily accessible for both beginners and experienced developers.What inspired me to start this project was the fascinating developments in the field of robotics and the sharing of open-source communities. I also believe that this project can contribute to the popularization of robotic technology. Making robotic technology more understandable and accessible to a wider audience can open doors to future innovations.The decision to start working on this project stems from my interest in robotics and my desire to contribute to technology. Additionally, the collaboration of open-source communities, individuals like Dan, and the sharing of knowledge, motivates me. Working towards the success of this project and increasing my contributions to the field of robotics is what inspires me.What motivated you to use the modular robotics framework for your project?Emir meeting the president of the province where he livesIn the beginning, we can see that this software’s modular structure is evident from its name. Specifically, the advantages of running the entire system with a library logic from a main script can be discussed. This approach lightens the computational load and reduces complexity. For example, this framework has a folder named “modules,” and each module has its own library (e.g., “tts.py” or “neopix.py”). The main script imports these libraries as follows: “from modules.neopix import NeoPx.” This way, we import the relevant library to use the functionality of each module.Secondly, a single processor or board can lead to problems like freezing or lagging. However, this framework utilizes both a brain (main computer) and a development board (like Arduino). This minimizes issues such as freezing and lagging. In terms of task distribution, the brain makes all decisions, carries out functions like hearing and vision, and sends signals. Arduino, on the other hand, directs the legs based on incoming commands, much like the way human beings function.Thirdly, this framework is versatile and entirely customizable. For instance, in the original repository, Raspberry Pi and Arduino Pro Mini are used, but in your version, you can use different components like NVIDIA Jetson Nano and Arduino Pro Micro. This broadens the application areas of the framework and enhances its functionality. It can be used in various areas, such as a desktop companion, home security, car camera, recording family memories, timelapse photography, an assistant (e.g., a legged Alexa), pet monitoring, a ChatGPT chatbot, creating memories for a time capsule, and many more.Finally, the software contains a variety of different modules, greatly enhancing the framework’s functionality. For example:  Actuators: Controls servo motors.  Behaviors: Contains command files that can be thought of as personalities, such as “dream,” “feel,” “motion,” and “respond.”  Image Processing: Computer vision modules related to Google Coral or OpenCV and Python.  Chatbot: Can be used to create a chatbot.  Melody Playing and Morse Code: A module for playing melodies with a buzzer and speaking in Morse code.  Serial Communication: Supports serial communication with Arduino.  Speech Recognition: Including an optional Snowboy hotword module.  Animation: Used to control animations.  Battery Status: Monitors the battery’s current status and provides alerts for critical conditions.  Other Modules: Includes BTWrapper, LogWrapper, Microwave Radar, Keyboard and Gamepad Control, Neopixel Control, RPi Temperature Monitoring, Speech Input, Translation, and Power Management, among others.This framework provides a flexible and customizable foundation for users to use in various projects.What customizations have you made to the framework to suit your project’s needs and your creative vision?I have made several customizations to the framework to align it with the needs of our project and our creative vision. Here are the key changes I’ve implemented:      Camera Integration: I added an older action camera to the system, which is connected to the project via USB with the Nano to provide visual input.        Additional Modules: I introduced various modules to enhance functionality within the project. These modules include a separate RFID module, stereo audio output, speakers on the right and left sides, and a panel providing additional audio output.        Charging Section: To further improve the charging section of the system, I added two Type-C connection points and a DC barrel jack to the back. I plan to add USB B and USB A outputs to this section, making it possible to charge external devices like smartphones.        Layout Change: I repositioned the Arduino Pro Micro and MPU6050 within the project to optimize its design.        Cable Management: To prevent cable clutter within the leg model, I improved cable management by using 3D model drag chains.        Overhead RCW Module: I developed a model that can move overhead, thus enhancing the project’s mobility.        I2S MEMS Microphones: I strategically placed I2S MEMS microphones within the head for audio input and output. There’s one on the right side and one on the left side. Additionally, the buzzer LLC buzzer is integrated into this section. This arrangement ensures that the audio output corresponds to the input direction. For instance, if the input is coming from the right microphone, the system will play the sound through the right speaker.        Bluetooth Wristband Controller: I created a controller with a 16x2 display, ESP32, and 5 potentiometers to offer an additional control option for the robot.        Exohand: I added a separate module that allows controlling the robot using gestures with a kind of glove.  These customizations were vital in making the project align with our specific needs and creative vision, and they have significantly improved the overall performance and functionality of the project.Could you describe a moment during this project when you felt particularly excited, proud, or motivated by your progress or a breakthrough?Moments of Excitement and PrideWhen I first started the project, I was particularly excited. The most significant moment of excitement was when I began the project. Of course, besides this excitement, there were other moments that got me excited. For instance, I was thrilled when the mayor of our city visited our village, and I explained the main concept of the project and the changes I made. Another unforgettable moment was when I traveled to Ankara with my cousin, who is a faculty member at Middle East Technical University (METU). While at METU, I was excited to talk to mechatronics and software engineering students about Dan, my own project, and Dan’s main project. I felt proud of myself after these moments, knowing that I had contributed to the project.Moments of Motivation and ProgressThere were certain factors that motivated me during the project. Firstly, the idea of creating, testing, and even improving one of the early versions of an open-source project was a great source of motivation for me. During this process, self-improvement was equally important.The outcomes of the features I added motivated me. Among the features that set me apart from the main project and formed my own branch were RFID, ChatGPT, Bluetooth controller, MPU6050, and the module that makes audio output correspond to the direction of the audio input. For instance, if the sound is coming from the right microphone, the system will play the sound through the right speaker, and during this time, the robot will look to the right. I am working to add some of these features to the main project in collaboration with Dan. The development of this project and my contribution to it motivate me.Moments of Breakthrough and ChallengesUndertaking such a project at a young age and developing my own projects may not be something everyone in my country can or will do. Heavy school workloads, time constraints, and the responsibilities of living in an earthquake-prone area made it difficult for me to allocate time to my projects. As a student, I used to go to school at 11 AM and return home at 7 PM. I couldn’t find time for my projects, so I had to shorten my sleep time. Over time, I got used to it, and sleeping 1 or 2 hours a day stopped being an issue for me. That’s because my projects and science are very important to me. One of the characteristics that sets me apart from others is my dedication to my projects and to science, regardless of any challenges. Projects and science should progress and continue to evolve, no matter what. This is the most important source of motivation for me.Were there any unexpected challenges or setbacks you faced either with the project or on your robotics journey in general?Getting Started and MotivationWhen starting the project, I realized that the source of the difficulties I faced in getting started was not related to the project or the framework, but rather to the act of starting itself. The beginning stage is always contentious, but I would always remind myself, “Just start somehow in any area, because starting is half the success.” The most challenging aspect of this project for me was related to mechanics. Lately, the issue we’ve been working on the most with Dan and the community is getting the robot to walk in a balanced manner. Currently, the robot was falling while walking, and we are working intensively and seeking solutions to solve this problem.Lack of Knowledge and the FrameworkAnother problem was my lack of knowledge about robots. I needed to find a good and simple framework to learn about this subject. That’s when Dan’s framework came into my life. This framework made me feel like it’s a perfect resource for both a beginner and an experienced developer.First Step into Robotic TechnologyThe first time I started learning about the structure of the robot, I took my first step into robotic technology using the Companion-Robot framework. This allowed me to enter the world of robotics.Hitches and Quests for SolutionsSpeaking of hitches with the robot, as I mentioned above, the biggest challenge for me was the walking aspect of the robot. I conducted a lot of research on this and came up with ideas. For example, I considered adding electromagnets under the feet, thinking that when a step is taken, the magnet would come into play and prevent the robot from falling. However, I abandoned this idea due to the issue of drawing high current. I also thought about adding strong springs to the foot model, but I gave up on that idea because it could put excessive strain on the PLA body structure. I am still working on this issue and hope to find a solution soon.Can you tell us about any communities or resources that have been helpful to you?I don’t have a lot of major support resources, but the most significant ones for me are research and the people in the Maker Forge community, and of course, Dan. Dan is supportive in many aspects, whether it’s related to robots or not.When my resources are limited, I have to do research to access knowledge. When I encounter a situation I don’t understand from sources like YouTube and ChatGPT, I delve into research. However, I don’t have blind faith in the information I receive. I always question the information and act based on the outcome.What plans do you have for the future?My future plans are a bit complex because I’m interested in both Software Engineering and Mechatronic Engineering. However, in the end, I want to become a great software engineer, while Mechatronic Engineering remains a hobby, and I aim to become a “maker.”After graduating from high school, my goal is to attend one of Turkey’s top universities, such as Bahçeşehir University, to receive an education in software engineering. One of the reasons I choose this university is because it offers a blue diploma, which can increase my chances of pursuing further education abroad.One of the reasons I consider studying abroad is my dream of working or even relocating to countries like the UK, Switzerland, and Finland after becoming a software engineer. During the job search phase in these countries, I would love to have the opportunity to work for companies like Google and Riot Games.Alongside all these goals, I also aim to contribute to the advancement of science by creating open projects on GitHub and helping future generations.What advice would you give to other members of the maker community who are thinking about starting their own robotics projects?My advice to anyone considering starting their own robot projects is to focus on research and work. Research findings can convince you that everything that initially seems impossible is, in fact, achievable. This comes from my own experiences.However, research alone is not enough; work is essential. I would like to conclude this piece with the saying, “When you rest, You rust.” Consider this saying as if it were a cog in a machine. If a cog stops, it rusts, and that cog is you. Therefore, hard work is of utmost importance in every project."
  },
  
  {
    "title": "Git for Makers | Guides",
    "url": "/posts/git-for-makers/",
    "categories": "Guides, Git",
    "tags": "github, git",
    "date": "2023-10-20 00:00:00 +0100",
    





    
    "snippet": "Many makers use software or 3D models as part of their projects and need a safe way to store those files and share them with others. Git is a great tool for this and is used by many makers and deve...",
    "content": "Many makers use software or 3D models as part of their projects and need a safe way to store those files and share them with others. Git is a great tool for this and is used by many makers and developers around the world. Let’s take a look at how the basicsImage credit: GitWhat is Git?Git is software that allows us to track changes to files and folders. It is used by many developers and makers around the world to store their code and share it with others.This type of software is known as a version control system and is the standard approach to working collaboratively in the software development industry.A project location is called a repository or repo. You’ll see that mentioned a lot.What is GitHub?GitHub is a website that allows us to host our git repositories online. It is a great way to share our code with others and to collaborate on projects. It also allows additional functionality, such as sharing and reviewing code.Not to get too technical, but you don’t need to use GitHub in order to use git. You can think of GitHub as an add-on that facilitates the online storage and sharing of your repositories. If you wanted to you could create a repository on your local machine, but GitHub makes the creation and sharing of repositories much easier.Are there other platforms like GitHub?Yes! There are many competing services that offer similar functionality. The most popular are:  GitLab  BitBucket  SourceForgeThere are also other version control systems besides git! You can assess the features and benefits yourself to determine which you’d prefer to use. For this guide we’ll be using GitHub.How do I get started?Create a GitHub accountFirst you’ll need to create a GitHub account. You can do that hereThe website will guide you through the process of creating an account. Choose your username carefully as it will be visible to other users.Create a repositoryOnce you’ve created your account you’ll need to create a repository. This is where you’ll store your files.Use this option if you want to create a new repository from scratch to store files for your project.In the top right corner of the GitHub website you’ll see a + icon. Click this and then select New repository. Enter a name, this will be visible by other users. You can also add a description if you like.Contributing to an existing repositoryAnother approach to working with Git is to be invited to join an existing repository.This allows you to make changes to the repository and then submit those changes to the owner of the repository for review and approval.Often this is for teams working together with equal access. For example, a team of developers working on a software project.Creating a copy of an existing repositoryIf you aren’t authorised to work on an existing repository and want to create a copy, you can do that by clicking the Fork button on the repository page. This creates a copy of their repository (including any change history) in your account, which you can then edit and make changes to.From this point forward, the repository is yours and you can make any changes you like. You can also submit a Pull Request to the original repository owner to request that they merge your changes into their repository (see below).This is a great way to contribute to open source projects and to share your changes with others, as you can work independently and choose which changes are merged into the original repository (if any).What is a Pull Request?In GitHub, a Pull Request is a request to merge changes from one repository into another. This is typically used when you want to merge changes from a forked repository into the original repository.They are also used in a repository to merge changes between branches.And what are branches?I know, it’s getting a little complicated, but bear with me. Branches are just a way to organise your code. You can think of them as folders that contain a specific version your code.Typically a developer will create a new branch from the main branch (often called develop, main or master), this effectively copies all the files and the change history into a new ‘folder’. They can then make changes to the code in that branch without affecting the main branch.Anything you do in a branch is isolated from anyone else’s changes, so it’s safe. You can delete the branch if you want to delete your changes, or you can request that your changes be pulled into the main branch (a ‘pull request’, see?).Branches are typically used so that developers can work on a feature in isolation, then merge that feature to the main code when it’s finished (or at least finished enough!).The advantage is that a team can work on multiple features at once, so you could have branches for feature 1, feature 2 and feature 3 and each developer can work on their own feature without affecting the others. When they are finished they can merge their changes into the main branch.What else do I need to know?As far as the basics go, you’re nearly there.Something you need to remember about Git and GitHub is that, as you’re working on your local machine, you need to push your changes to GitHub in order to sync them with the website. This is a good habit to get into, as it means that your changes are backed up and you can easily share them with others.You’ll see mentions of ‘committing’ changes and ‘pushing’ them. That’s all this means. If you commit a change to a branch, that change is saved to your local copy of the repository. Pushing the change then syncs that change to GitHub.Similarly, you may find that you need to ‘pull’ changes from GitHub if you have other contributors making changes on the branch. This makes sure you have their changes on your local machine.Is it safe?If you create a private repository, the contents won’t be accessible to anyone unless you give them access (one of the great features that GitHub offers) If you create a public repository, anyone can see the contents.In either case, it’s bad practice to include any secrets in your repository, this means passwords or other sensitive information, you should make sure you’re not storing them in your repository so they don’t get shared with others.Let’s go!GitHub DesktopThe GitHub Desktop app is a great way to get started with git. It’s a graphical interface that makes it easy to create and manage repositories. You can download it here.Take a look at the guideThe official guide will take you through the steps below using GitHub Desktop:  Installing the app  Signing in to GitHub  Creating a new repo or cloning a repo (see above)  Making changes in a branch  Collaborating  Syncing your changesUsing the CLIMany seasoned developers (including myself) like to use the command line interface (CLI) to interact with git. This is a great way to learn the basics and to get a feel for how git works. It also means you can work with git quickly an easily once you get used to the commands.You don’t need GitHub Desktop installed to use the CLI, but you will need git and a GitHub account.This is a little more complicated than I want to cover in this article, but if you’re comfortable with getting set up yourself, I’ve added some examples in a table below of how to use the CLI with GitHub.            Command      Description      Example                  git init      Initialise a new git repository locally (an alternative to using GitHub)      git init              git clone      Clone an existing repository      git clone https://github.com/dmt-labs/modular-biped.git              git add      Add files to the staging area      git add --all              git commit      Commit changes to the local repository      git commit -m \"My commit message\"              git push      Push changes to the remote repository      git push              git pull      Pull changes from the remote repository      git pull              git branch      Create a new branch      git branch my-new-branch              git checkout      Switch to a different branch      git checkout my-new-branch              git merge      Merge changes from one branch into another      git merge my-new-branch              git status      View the status of the changes locally      git status              git log      View the commit history      git log      FAQThere are quite a few quirks you’ll need to know about using Git and GitHub. I’ll add some of my most common questions here.Where do I find the URL to clone a repository?You can find the URL to clone a repository by clicking the green Code button on the repository page. This will open a menu where you can copy the URL. Enter that into your clone command and you’re good to go!How do I add files to my repository?You can add files to your repository by dragging them into the GitHub desktop app. You can also use the command line to add files to the staging area using the git add command.If you’re using the command line you can use git add . to add all files in the current directory, or git add --all to add all untracked files in the repository. Similarly you can specify the file specifically with git add myfolder/file.txt if you want to have a little more control.How often do I need to sync (pull and push changes)The answer to this depends on how the repository is being used. If you’re the only person contributing then you’ll likely never need to pull changes, unless you’re pushing from multiple locations. If you’re working with others, you should pull whenever there are changes made that you want to include in your branch.You should push your changes frequently, as this will ensure that your changes are backed up and you can easily share them with others.What is a conflict and how do I resolve them?A conflict occurs when two people make changes to the same file at the same time. Git will try to merge the changes automatically, but if it can’t it will ask you to resolve the conflict manually.Usually that looks like this:&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEADThis is the original text=======This is the new text&gt;&gt;&gt;&gt;&gt;&gt;&gt; my-new-branchThe text between &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD and ======= is the original text, the text between ======= and &gt;&gt;&gt;&gt;&gt;&gt;&gt; my-new-branch is the new text.Note: Your editor may highlight this for you, and give you an option to accept the current or incoming change, or accept both.You can choose which text you want to keep and then delete the other text and the conflict markers. Once you’ve done that you can commit the changes and push them to the remote repository.When you’re working with others, you may find that you need to resolve conflicts often. This is normal and is part of the process of working collaboratively. Please PLEASE don’t just delete the other person’s changes in favour of your own. That’s now how you make friends."
  },
  
  {
    "title": "Teaching A Robot To Walk - Part One",
    "url": "/posts/reinforcement-learning-intro/",
    "categories": "Robotics, Guides, Reinforcement Learning, Simulation",
    "tags": "robotics, walking, biped, reinforcement-learning",
    "date": "2023-10-18 00:00:00 +0100",
    





    
    "snippet": "Following on from my recent post discussing examples of walking robots, it became clear that a logical next step would be to look at how we can teach a robot to walk using simulators and reinforcem...",
    "content": "Following on from my recent post discussing examples of walking robots, it became clear that a logical next step would be to look at how we can teach a robot to walk using simulators and reinforcement learning.Let’s take a look at these concepts and how they can be applied to the Modular Biped project.What is a simulator?A simulator is a piece of software that allows us to create a virtual environment that we can use to test our robot. This is useful for a number of reasons:  We can test our robot without having to build it  We can test our robot without having to worry about breaking it  We can test our robot in a variety of environments  We can test our robot in a variety of scenarios  We can run thousands of tests in a short period of timeThis last point is particularly important when we are using reinforcement learning. We need to be able to run many tests in order to train our robot to walk. This would be very difficult to do in the real world, but in a simulator we can do this as long as we have the processing resources available.What is reinforcement learning?Reinforcement learning is a type of machine learning that uses a reward system to train a model. The model is given a goal and then rewarded for achieving that goal. The model is then able to learn how to achieve the goal by trying different actions and observing the results.In the case of a walking robot, the goal would be to move from one location to another. The model would be rewarded for moving towards the goal and penalised for moving away from the goal. This would allow the model to learn how to move towards the goal.It is also possible to reward or penalise the model for other actions, such as using too much energy to walk, or changes in direction of servos. This allows us to train the model to walk in a way that is efficient and smooth.How can we use these concepts in the Modular Biped project?The Modular Biped project is a great candidate for this type of learning as it is a complex task that is difficult to program manually. It is also a task that is difficult to test in the real world as it requires a lot of time and resources to build and test.The current structure of the robot does not allow for lateral hip actuation (although there are joints in the hips) and so we will need to add this functionality in order to allow the robot to walk. Rather than building the hardware to test, we can assemble it virtually in a simulator and then test it in a virtual environment. This allows us to iterate the design much more quickly than in the real world.How do we create a simulator?Fortunately for us, there are a number of simulators already available, either free or for a fee. These include:  Gazebo  Webots  V-REP  Unity  Unreal Engine  PyBullet  Mujoco  OpenAI GymBecause I have no idea what I’m doing, and because I have a guide for Mujoco, I will be using that as an example. However, the concepts are similar for all of the simulators listed above.What is Mujoco?Mujoco is a physics engine that allows us to create a virtual environment and simulate the physics of that environment. We can use it to define the structure of our robot and the properties of the materials that it is made from. We can also define the properties of the environment, such as gravity and friction.Let’s get startedFor this quick example I’ll be using the repository from GitHub user @sergiynesterenko90View Original RepositoryMaker Forge forkIt utilises Google Colaboratory (colab) workspaces to run the simulations. This is a great way to get started as it allows us to run the simulations without having to install anything locally. It also allows us to use the free GPU resources provided by Google to speed up the training process.Step 1 - Create a Google Colab workspaceOpen the URL above and navigate to the notebooks folder, then open the brax.ipynb notebook.At the top of this notebook you will see a button that says ‘Open in Colab’. Click this button to open the notebook in a new tab.Step 2 - Run the notebookYou will see a list of steps in the notebook. These are the steps that we will follow to train our robot to walk. Each must be executed sequentially in order to train the robot.  Import dependencies  Load environment  Training  Saving and Loading policies  Visualizing a trajectory of the learned inference functionWhen training the model you should see a chart detailing the reward per episode and number of environment steps. This shows the progress of the training process.After visualising the trajectory you should see the outcome of the reinforcement learning process. This is the robot learning how to walk.Learn MoreThe YouTube video below shows the process in a lot more detail, explaining the use of all the notebooks detailed in this repository. It’s a long watch but worth it for the additional information and context.What now?This is a great first step, but to continue development on your own you will need to model your own robot. This is where the Mujoco documentation comes in handy. You can find it here:MuJoCo DocumentationIn the next part of this series I will be using this documentation to create a model of the Modular Biped robot and then (hopefully) train it to walk. If you’d like to help out please get in touch on the community"
  },
  
  {
    "title": "Let's Learn About Walking Robots",
    "url": "/posts/walking-case-studies/",
    "categories": "Robotics, Modular Robotics Framework, Build Log",
    "tags": "robotics, walking, biped",
    "date": "2023-10-14 00:00:00 +0100",
    





    
    "snippet": "The biggest question I get asked when showing my bipedal robot to people is is “Does it walk?”.The answer is ‘no’, or rather, ‘not yet’.In order to understand the problem, we need to understand the...",
    "content": "The biggest question I get asked when showing my bipedal robot to people is is “Does it walk?”.The answer is ‘no’, or rather, ‘not yet’.In order to understand the problem, we need to understand the basics of walking robots and the challenges that they face. This post will talk about a few examples available today.Image credit: MathWorksA word on actuatorsWhen we talk about actuators, we are talking about a components that move the robot. This could be a motor, a servo, hydraulics, a solenoid or even a muscle.Generally there are three that are used in robotics for mobility:  DC Motors - These are the most common type of motor used in robotics. They are cheap, easy to control and can be found in a wide range of sizes and power ratings. They are also very easy to control using a microcontroller such as an Arduino or Raspberry Pi. The downside is that they are not very precise and can be difficult to control at low speeds. A variation on this is a BLDC (Brushless DC) motor which is more efficient and can be more precise, but requires a more complex control system.  Servos - Servos are a type of motor that includes a gearbox and a position sensor. This allows the motor to be controlled to a specific position and hold that position. Servos are very common in robotics and are used in many hobbyist projects. They are easy to control and can be very precise.  Stepper Motors - Stepper motors are a type of motor that can be controlled to move a specific number of steps (a degree of rotation). This also allows the motor to be controlled to a specific position and hold that position. Stepper motors are very common in robotics and are used in many hobbyist projects. They are typically quite bulky and require additional control circuitry.Boston Dynamics Spot and other quadrupedsBoston Dynamics are a robotics company that have been working on walking robots for many years. They have a number of robots that are capable of walking, including the famous Spot robot. Although there are now many examples of quadruped robots, Spot is one of the most well known.Quadrupeds typically use DC motors combined with a gearbox to provide the power and precision required to move the legs. Gearboxes such as the planetary gear system are used to increase the torque of the motor and to reduce the speed. This allows the motor to be more precise and to move the legs at a slower speed.Image credit: WikipediaThe motors are typically mounted in the body of the robot and connected to the legs using a series of linkages. This allows the motors to be placed in a central location and the legs to be extended outwards.Image credit: WikipediaThe quadruped robot is a great example of a robot that can become statically stable. This means that the robot can stand still without falling over. This is because the legs are spread out and the center of gravity is low. This is a great advantage for a robot that is designed to move around and interact with the world.The current version of the modular biped is also statically stable, but in order to make it walk we will need to make it dynamically stable. This means that the robot will be able to balance itself while moving. This is a much more difficult problem to solve.Let’s take a look at some other examples.Soby - Bipedal wheeled robotSoby has a very similar form factor to the modular biped. One of the key differences is it’s mobility, which is possible because it has wheels instead of feet.As you can see in the video, the wheels allow the robot to balance and move about the room. It is also able to move at a much faster speed than a walking robot.The disadvantages to this approach is that, when not moving, the robot is still using power to balance itself. This is not a problem for a robot that is designed to move around, but for a robot that is designed to sit on a desk and interact with the world for the majority of it’s time, this is not ideal.We could look to be able to ‘park’ the robot, so that a resting position allows it to be dynamically stable, and then it can be ‘activated’ to move around the room. This may be something that we look at in a future variation, but for now we will focus on a walking robot.Dynamic locomotion (MIT)This robot attempts to mimic the movement of a human. It uses a combination of DC motors and linkages to move the legs as I described above. It is able to balance dynamically, even with changes to the ground plane it is standing on.The motors and gearboxes appear to be located solely in the ‘hips’, with belts to actuate the knee. This reduces the weight of the legs, allowing faster movement by reducing the weight of the legs.Using BLDC motors is definitely an option, although the cost and complexity will reduce accessibility, which is something I am keen to avoid in this project.Birdbot - BiomimicryThis robot from Max Planck Institute for Intelligent Systems is a great example of biomimicry. It uses servos to actuate the legs combine with very clever system of springs and pulleys to mimic the movement of a bird. This allows for repeatable motion using fewer servos than with other walking robots.Again, the complexity to achieve this is high, which would reduce the accessibility of the project. It is also not clear how the mechanisms behave in a resting pose or when turning.I would be very interest to see other systems using this approach in future.Parallel mechanismsParallel mechanisms are a type of mechanism that uses multiple actuators to move a single point. This is useful for robotics as it allows for a more compact design and can reduce the number of actuators required.A good example of this can be seen below, from Disney Research.This robot has great range of motion on each leg and is able to achieve a consistent walking gait.Simulations to train and testThere are a number of simulation tools available to help with the design and testing of walking robots. These allow faster iteration of the design and can help to reduce the cost of prototyping.Simulations can also be combined with reinforcement learning to train the robot to walk in an environment that can be simulated and reset quickly. This is a great way to train the robot to walk without the risk of damaging the hardware, it also allows for thousands of simulations to be executed in the time that it would take to do a single test in the real world.This robot shows this process in action on a small bipedal robot.Disney Research also recently released footage of a robot that takes this approach to the next level. The robot was not only trained to walk with reinforcement learning, but was also trained to behave in certain way that appears animated.If you’re interested in reinforcement learning, mathworks has a detailed e-book that covers the basic concepts:MathWorks - Guide to Understanding Reinforcement LearningWhat does it all mean?With such a wide range of options available for mobility of the modular biped, it would make sense to utilise simulations to attempt to train the robot to walk.Reinforcement learning is also an interesting avenue to investigate as it would allow the robot to learn to walk without the need for complex manual programming. This would also allow the robot to adapt to changes in the environment, such as a change in the ground plane or the addition of an obstacle.I will likely investigate this approach in the future, so if you have suggestions for a simulation tool or reinforcement learning framework, or other examples of mobile biped robots, please let me know in the community."
  },
  
  {
    "title": "Using Raspberry Pi 5 with Modular Biped",
    "url": "/posts/pi-5-deep-dive/",
    "categories": "Robotics, Modular Robotics Framework, News and Events",
    "tags": "robotics, news, raspberry-pi-5",
    "date": "2023-10-10 00:00:00 +0100",
    





    
    "snippet": "The Raspberry Pi 5 was announced last month and offers a number of improvements over the model currently used in the Modular Robotics Bipedal Robot project (The Pi 3B+). Let’s do a comparison to se...",
    "content": "The Raspberry Pi 5 was announced last month and offers a number of improvements over the model currently used in the Modular Robotics Bipedal Robot project (The Pi 3B+). Let’s do a comparison to see what kind of features and issues we’d expect to see when using the new model.Image credit: Raspberry PiRaspberry Pi 5 vs Raspberry Pi 3B+ (Relevant Features)            Feature      Pi 5      Pi 3B+                  USB      2x USB 3.0, 2x USB 2.0      4x USB 2.0              Camera      2x CSI camera ports      1x CSI camera port              GPIO      40-pin GPIO header      40-pin GPIO header              Power      USB-C, 5v 5A (Max)      Micro USB, 5.1v 3A (Max)      Power requirementsFrom the power requirements, we can see that the Pi 5 requires a USB-C power supply, which is a welcome change from the Micro USB connector on the Pi 3B+. This is a more robust connector and is less likely to break over time. Fortunately this doesn’t impact the project as we connect the power directly to the 5v rail using the custom PCB and a DC-DC converter within the head of the robot.One challenge would be to make sure that the DC-DC converter can produce the number of amps required for the new model. For example, the XL4015 is capable of providing 5A, but the lesser converters XL4005 or MINI360 that I have used in the past are only capable of 3.5A or 1.8A respectively. Fortunately the form factor of the XL4015 is almost identical to the XL4005 currently in place.Outcome: Upgrade DC-DC converter to ensure 5V 5A is available to the Pi 5 (XL4015 is compatible)Form FactorThe Raspberry Pi 5 shares a similar form factor to the Pi 3B+, with the same 40-pin GPIO header and the same mounting holes. This means that the existing PCBs and 3D printed parts should be compatible with the new model. Although the ethernet and USB ports are in different locations to the Pi 4, it seems that are in the same position as the Pi 3B+.Image credit: Raspberry PiOne other consideration is that the Pi 5 requires a fan to keep it cool, which is not the case for the Pi 3B+. This means that the custom PCB may not fit as the fan may be blocking components. This will need to be tested.Outcome: The Pi 5 should be compatible with the existing PCBs and 3D printed parts, but the fan module may block the PCB from being installed.CameraThe Pi 5 has two CSI camera ports, which is a welcome addition as it allows for the use of two cameras for 3D imaging and localisation. This is something that I have been looking into for a while, but the Pi 3B+ only has one camera port which limits the options.The use of a second USB camera has always been possible, but this is not ideal for this project where anything connected to the USB ports is highly visible.Image credit: Raspberry PiI’m looking forward to exploring the use of two cameras for this project and expect other makers will be doing the same.Outcome: The Pi 5 has two MIPI CSI camera ports which will allow for the use of two cameras for 3D imaging and localisation.Power ButtonTo the relief of many makers, a power button has been added to the Raspberry Pi 5 to enable a safe shutdown and restart option on the single board computer. This is a welcome addition as it means that the robot can be shutdown safely without having to SSH into the Pi or use a physical switch.Currently the modular biped project has support for an external shutdown switch which, when pressed, triggers an immediate shutdown of the running python and then Raspbian OS. Without this support the SD card can become corrupted and the Pi may not boot up again if the power is disconnected while running.The addition of the power button may render the external switch redundant, but this will depend on the accessibility of the button when the robot is assembled.Outcome: The external shutdown switch may no longer be required, but this will need to be tested.Increased CPU and RAMThe current version of the project relies on a Coral USB Accelerator to perform the object detection and recognition. This is a USB device that contains a TPU (Tensor Processing Unit) which is designed to perform machine learning tasks. This connects to the Pi 3B+ via USB and is used to offload the processing of the object detection and recognition to the TPU, which is much faster than the CPU on the Pi.Without this device the Pi 3B+ would struggle to perform the object detection and recognition in real time, but the Pi 5 has a much more powerful CPU and GPU which may be able to perform these tasks without the need for the Coral USB Accelerator.Removing the USB Accelerator would reduce the cost of the project and save power, which would help reduce the impact of swapping the Pi 3B+ out for the more power hungry Pi 5.Outcome: The Pi 5 may be able to perform the object detection and recognition in realtime without the need for the Coral USB Accelerator.Audio JackThe Raspberry Pi 5 does not have an audio jack, and the current project uses a speaker module that relies on this jack for output.However, the same i2s connection that is used for the MEMS microphones may be compatible with an i2s speaker amplifier module. This will need investigation.Outcome: Investigate i2s speaker amplifier module to replace the current module.ConclusionThe Raspberry Pi 5 is a welcome upgrade to the Pi 3B+ and offers a number of improvements that will benefit the Modular Biped project. The main benefits are the dual CSI camera ports, the increased CPU and RAM, and the power button.For very little effort, the project could be upgraded to use the Pi 5 and this would allow for the use of two cameras for 3D imaging and localisation. The increased CPU and RAM may also allow for the removal of the Coral USB Accelerator, which would reduce the cost and power consumption of the project.I will be testing the Pi 5 with the Modular Biped project as soon as I can get my hands on one, so watch this space!"
  },
  
  {
    "title": "Reddit Robotics Showcase 2023",
    "url": "/posts/reddit-robotics-showcase/",
    "categories": "Robotics, News and Events",
    "tags": "robotics, news, reddit",
    "date": "2023-10-09 00:00:00 +0100",
    





    
    "snippet": "Earlier in the year I was invited to speak about the Modular Robotics Framework on session 2 of the Reddit Robotics Showcase. This was a great opportunity to share the project with a wider audience...",
    "content": "Earlier in the year I was invited to speak about the Modular Robotics Framework on session 2 of the Reddit Robotics Showcase. This was a great opportunity to share the project with a wider audience and to talk about the history of the project and the future plans.The playlist for the entire showcase is now available on YouTube and linked below. There were a lot of really interesting projects, so please feel free to watch the entire series.Reddit Robotics Showcase 2023The showcase includes:  Session 1: Robot Arms  Session 2: Social, Domestic and Hobbyist Robots  Session 3: Autonomous Mobile Robots  Session 4: Startups and SolutionsThe keynote for session 2 was Eliot Horowitz, CEO and Founder of VIAM. Eliot is also the founder of MongoDB, a database platform used by many companies around the world and joined the live stream to talk about how VIAM is helping to make robotics more accessible.You can see the original program here to see the full list of speakers and projects."
  },
  
  {
    "title": "September Digest | Maker Forge",
    "url": "/posts/september-digest/",
    "categories": "Robotics, Modular Robotics Framework, News and Events",
    "tags": "robotics, news, raspberry-pi-5",
    "date": "2023-10-08 00:00:00 +0100",
    





    
    "snippet": "Featured on Official Raspberry Pi NewsI was very excited to see that the Modular Biped project was featured on the official Raspberry Pi news page. This is a great honour and I’m very grateful to t...",
    "content": "Featured on Official Raspberry Pi NewsI was very excited to see that the Modular Biped project was featured on the official Raspberry Pi news page. This is a great honour and I’m very grateful to the Raspberry Pi team for the recognition. The article focussed on the background of the project and the history of the development.You can read the article here: Build a bipedal companion robot - Raspberry Pi NewsFinalist in the 2023 RealVNC Raspberry Pi PrizeThe Modular Biped project was selected as one of six finalists in the 2023 RealVNC Raspberry Pi Prize.The project has been chosen among other finalists such as a conveyor belt monitoring system, a guitar playing robot and even another companion robot!All the projects are very impressive and worth reading about, take a look at the article below.RealVNC Raspberry Pi Prize finalists - RealVNC BlogA more detailed article about each finalist (including the Modular Biped project) will be posted over the coming weeks on the RealVNC blog.RealVNC BlogLaunch of the Maker Forge websiteAfter looking for a way to share my projects and ideas, a conversation with Kevin Mcaleer prompted me to create a website to host my projects and blog posts. I wanted to create a place where I could document my progress and learning as well as share guides and news from the wider maker community.If you’re reading this then it means you are here, so welcome! I hope you enjoy the content and find it useful. If you have any questions or suggestions, please feel free to get in touch via the discussion group.Raspberry Pi 5 AnnouncedThe Raspberry Pi 5 was announced on the 28th September 2023. This is a very exciting announcement as it brings a number of improvements over the previous model, including support for dual CSI cameras which may be beneficial for 3D imaging and localisation.I’m looking forward to the release of the new model and will be testing it with the Modular Biped project as soon as I can get my hands on one.https://www.raspberrypi.com/news/introducing-raspberry-pi-5/"
  },
  
  {
    "title": "Modular Biped Features | Build Log",
    "url": "/posts/modular-biped-features/",
    "categories": "Robotics, Build Log, Modular Robotics Framework",
    "tags": "modular, robotics, framework",
    "date": "2023-10-05 00:00:00 +0100",
    





    
    "snippet": "When I first started working on the modular biped project, I had a few features in mind. The way that the robot behaved would depend entirely on the sensors and actuators it had in it’s arsenal. Fo...",
    "content": "When I first started working on the modular biped project, I had a few features in mind. The way that the robot behaved would depend entirely on the sensors and actuators it had in it’s arsenal. For that reason (and because I enjoy trying new things!) I decided to add as many as reasonably possible, as long as they were relevant to the brief of a ‘desktop companion robot’.Sensing the worldTo make the robot aware of it’s surroundings, I added a number of sensors. These include:Microwave sensor (RCWL-0516)This sensor has a digital output (true or false) and detects the presence of a human or animal within a few meters, even through walls. The sensor cannot be adjusted in sensitivity and so it was not useful to determine if a person was ‘in front’ of the robot, or even in the same room, but as a general ‘someone is near by’ detector it works well.Because the plan was to allow the robot to run autonomously and 24 hours a day, a sensor that detects people allows the robot to enter a power saving mode when no one is around. This is useful for a number of reasons, but primarily to reduce the power consumption and to reduce the wear on the servos.It is also a useful sensor for games and interactions, as it can detect if everyone is moving or still.It refreshes at a rate of once per second, which is sufficient for these purposes.Raspberry Pi Camera Module (wide angle lens)The camera is the main sensor for the robot. It has a versatile range of uses, including:  Face detection. When object detection detects a person, the mode switches to face detection so that the robot can focus on the face of the individual. The neck servos facilitate face tracking. If there is more than one face visible the robot will focus on the closest (largest) one.  Face recognition. This can be trained by collecting faces that the handler can label by placing into a named folder, at which point the robot can re-train the model. Note that this is available in the OpenCV functionality of version 2.  Object detection and recognition. This is useful for detecting objects when there are no faces to interact with. The robot simulates boredom by identifying objects from a list of pre-trained and focusses on each one for a few seconds before moving on to the next.  Timelapse and other capture features. The robot can be set to take a photo every few seconds, or to record video when it detects movement. This is useful for security and monitoring purposes.The wide angle lense is useful for the robot as it allows it to see a wider area, but it does distort the image. This is not a problem for the purposes of the robot, but it is worth noting.Microphones (MEMS)Image credit: AdafruitThe robot has 2 MEMS microphones, one on each side of the head. These are used for speech recognition but could be expanded to include other features such as sound detection and localisation.Speech recognition has been implemented with an ‘always listening’ approach, but when the robot enters rest mode this is paused. There is also the option to use a wake word, but this is not currently implemented as the module ‘chirp’ was used and is no longer maintained by the third party.Read MoreGyroscope (MPU6050)Built into the body circuitry is an MPU6050 gyroscope. This is used to detect the orientation of the robot and to determine if it is level or not. This is useful as the animation of the legs can throw off the center of gravity and cause the robot to fall over. The gyroscope is used to detect this and to correct the balance automatically in a continual loop.Interacting with the worldLED (Neopixels)Neopixels are addressable RGB LEDs that can be controlled individually or as a group.They are used to provide feedback to the user and to indicate the status of the robot.For example, when the robot is idle the eye LED is red. When the microwave sensor detects motion the eye changes to blue, and finally when the camera detects a person the eye changes to green.SpeakerA recent edition is a speaker module that connects directly to the audio output of the Raspberry Pi 3b+. This allows the robot to play sounds and music, as well as to speak using text to speech (TTS). The speaker is used to provide feedback to the user and to indicate the status of the robot.Image credit: The Pi Hut  Note: The Raspberry Pi 5 does not have an audio jack, but the same i2s connection that is used for the MEMS microphones may be compatible with an i2s speaker amplifier module. This will need investigationBuzzerPrior to adding the speaker, the plan was to enable the robot to emote and communicate with a buzzer in the head PCB, connected to the Raspberry Pi. Not only could this play simple tunes but with the addition of a piece of code it could also speak.Braillespeak (text to speech as tones)A few years prior to starting this project I had a concept for a way of using simple tones to output text. The concept was simple: split a word into each character, then convert each character into the ‘braille’ equivalent. This is a 6 bit binary number, which can be converted into a pair of tones. The tones can then be played in sequence to output the word.The advantage of this is that it is simple to implement and requires no additional hardware. The disadvantage is that it is slow and difficult to understand, but could be learned and would also allow for machine-to-machine communication over simple audio.This was disabled in the latest versions in favour of the audio output, but is still implemented in the code and hardware.Servos (animation, follow a face)I will need to write an article on the plethora of animation features that I attempted over the years whilst building this project, everything from being able to train an animation by recording mouse and keyboard input, to inverse kinematics for balance. There was even a dance mode at once point.The servos are an accessible choice for the project as they are reasonably cheap, easy to control, and don’t need an additional driver board. They are also very versatile and can be used for a number of purposes.The leg servos are standard servos SG5010 and the neck servos are MG92B, metal geared mini servos.In previous versions the entire robot worked with MG92B servos, as other mini servos such as SG90 or MG90 were not strong enough to support the weight of the robot.Calculating the weight your servos can handleYou can calculate the weight that your servos can support by taking the stall torque (found in the servo datasheet) and measuring the distance the weight is from the pivot point (the leg length, as an example).Torque is measured in kg/cm and is the weight the servo can support at a 1cm distance from the pivot point, so if you have a 3.5kg/cm servo and the weight is 2cm from the pivot point, the maximum weight it can support is half that value (1.75kg).If the weight is 10cm from the pivot point, the maximum weight it can support is 10%, or 0.35kg.Behaviours and PersonalityUsing the above sensors as inputs and actuators to facilitate interaction, the robot can be programmed to behave in any number of ways.I made the decision early on to create a state manager as part of the behaviour logic, so that the robot exhibits different behaviours depending on the state it is in.State ManagerThe diagram above shows a simplified overview of the state manager. Triggered by the wake event, we change the state to resting, alert or sleeping depending on input from the various sensors.The states determine the behaviour and running features. For example, in sleep mode the face and object detection is disabled.This can be configured to meet the requirements of any project.ConclusionThe software and behaviour is a topic that could be discussed at length, but I will leave that for another day. The above is a brief overview of the features and sensors that are available in the modular biped project."
  },
  
  {
    "title": "Disney Research Announcment | News",
    "url": "/posts/disney-research/",
    "categories": "News, Robotics",
    "tags": "robotics, bd-1",
    "date": "2023-10-05 00:00:00 +0100",
    





    
    "snippet": "Disney Research has announced a new robot at IROS2023 that looks suspiciously like the BD-1 robot from Star Wars Jedi: Fallen Order.This post from David Scaramuzza from the University of Zurich sho...",
    "content": "Disney Research has announced a new robot at IROS2023 that looks suspiciously like the BD-1 robot from Star Wars Jedi: Fallen Order.This post from David Scaramuzza from the University of Zurich shows the robot in action.A more detailed article on IEEE Spectrum explains the research behind the robot and the technology used to create it.Disney’s history of animatronic characters dates back to 1971, with the Hall of Presidents at Disney World, but making robots express emotions while moving convincingly has become increasingly challenging. To address this, Disney Research developed a new system leveraging reinforcement learning. The team, led by Moritz Bächer from Disney Research in Zurich, created the robot using 3D printing and modular hardware, allowing for rapid design iterations. It features a four-degree-of-freedom head and five-degree-of-freedom legs with dynamic balancing.Disney’s approach involves collaboration between animators and roboticists, combining artistic intent with technical expertise. The challenge lies in translating animations into real-world robot movements effectively. Disney Research’s solution is a reinforcement learning-based pipeline that simulates and balances the animator’s vision with robust robotic actions. This system significantly reduces the time required to develop a new robotic character from years to months.Reinforcement learning also leads to highly robust motions, enabling the robot to adapt to real-world scenarios while maintaining its character.The researchers emphasize that the key takeaway is not just the robot itself but the adaptable process. This platform is hardware-agnostic, allowing for the rapid development of new behaviors, even with different robot forms. Disney aims to explore physical robotic characters further in future.It’s great to see this kind of research being done in the field of robotics, and I’m excited to see what comes next.FAQ1. What is Disney Research’s recent innovation in robotics?Disney Research has recently unveiled an innovative robotic creation designed to convey emotions and character through its movements. This robot was showcased at the 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) in Detroit.2. What distinguishes Disney’s robot in terms of its movements?Disney’s robot stands out due to its ability to express a wide range of emotions through its movements. It goes beyond mere functionality, aiming to convey personality and character, from strutting to meandering, through dynamic and expressive motions.3. How was this robot designed and manufactured?The showcased robot is primarily 3D printed and was developed by a team from Disney Research in Zurich. Its modular hardware and actuators allow for quick design and iteration, making it a promising platform for further robotic developments.4. How does Disney’s robot achieve emotive movements?Disney has developed a reinforcement learning-based pipeline that combines animation and real-world physics to ensure that the robot can respond to its environment and maintain its character. This approach significantly reduces the time required to train the robot on new behaviors.5. How does this innovation impact human-robot interactions?Disney Research envisions a world where emotionally expressive robots, like this one, become invaluable allies in various fields, particularly in environments where they work closely with humans. By allowing robots to convey emotions and character, this technology has the potential to revolutionize and enhance human-robot interactions.6. How can makers and robotics enthusiasts benefit from Disney’s innovation?Makers and robotics enthusiasts can draw inspiration from Disney’s innovation to create robots that are not just functional but also emotionally expressive. Disney’s journey provides potential frameworks and methodologies for incorporating emotion and character into robotic designs.7. What is the significance of Disney’s hardware-agnostic approach?Disney’s approach is hardware-agnostic, meaning it can be adapted for robots with different designs and functionalities. This flexibility allows for the rapid development of a diverse array of robotic characters with unique characteristics and movements.8. What are Disney’s future goals in the field of robotics?Disney’s research team is eager to push the boundaries of what’s possible in robotics. While specific details about their future projects remain undisclosed, Disney Research’s journey with this innovative robot is an indicator of the exciting possibilities that lie ahead in the field.More references  Popular Science  The Verge  IEEE Spectrum  Medium"
  },
  
  {
    "title": "Maker Websites with GitHub Pages | Free Hosting",
    "url": "/posts/github-pages/",
    "categories": "Guides, Website",
    "tags": "github, webhosting, how-to",
    "date": "2023-10-03 00:00:00 +0100",
    





    
    "snippet": "As a maker that loves to show my projects to the world, one of the challenges I have always faced is understanding the best platform to do that.Many years ago I worked in web development. It was, b...",
    "content": "As a maker that loves to show my projects to the world, one of the challenges I have always faced is understanding the best platform to do that.Many years ago I worked in web development. It was, by comparison, much less accessible. I’m pleased to say that things have improved, now affordable and maintainable hosting solutions are everywhere.Enter GitHub Pages.What is GitHub Pages?  Not a fan of guides? Check out the TL;DR at the end.GitHub Pages is a free service that allows you to host a static website for free. It’s a great way to get started with a website and to get your projects out there. And best of all, you don’t need to know anything about web development to get started.GitHub Pages allows you to create static pages using Markdown, a simple markup language that is easy to learn. You can also use HTML if you prefer. GitHub Pages also supports Jekyll, a static site generator that allows you to create dynamic websites using Markdown and Liquid templates. Don’t worry if you don’t know these terms yet. We’ll get there!What about custom domains?By default you’d access your website from the URL username.github.io, but GitHub Pages also allows you to use a custom domain name. This means that you can have a website with your own domain name, like mydomainname.com. This is a great way to make your website look more professional and to make it easier for people to find you.If you’d like to explore this option now, you can find more information here.How do I get started?You’ll need to familiarise yourself with a few things to get this working. Don’t worry, it’s not as complicated as it sounds. I’ll walk you through the process step by step.  Take your time if you’re not familiar with these concepts. Understanding each step is important and will help you feel more confident in working with these tools.If you’re not familiar with using the command line (or terminal), you might want to take a look at a basic tutorial. That said, there is an app called GitHub Desktop that allows you to do most of this without using the command line, but I won’t be covering that here. I’ll leave it up to you to decide which you prefer.Install JekyllJekyll is a static site generator that allows you to create dynamic websites using Markdown and Liquid templates. It’s a great way to get started with a website and to get your projects out there. And best of all, you don’t need to know anything about web development to get started.Install JekyllCreate a GitHub accountThe first thing you need to do is create a GitHub account. If you don’t already have one, you can sign up for free at GitHub.com.Next you’ll need to set up git on your computer: Set up gitCreate a new repository from a themeWhat are repositories?Once you have an account, you can create a new repository. A repository is simply a location on your GitHub account where you will store your website files. It comes with some really nice features that let you track and manage changes and even let other people contribute.The naming of your repository must be username.github.io where username is your GitHub username. For example, if my GitHub username is makerforge so my repository name is makerforge.github.io. (Note: You can also set up organisations in github, but that’s a topic for another day.)Finding a themeGitHub pages supports a number of themes. These are pre-existing layouts that allow you to create a website without having to worry about the design. You can find a list of themes here. You can also create your own theme if you want to. As an example, this website uses the Chirpy theme. It comes with example pages and even the source code for the theme itself.If you want to use a theme, there are often starter repos that you can copy (fork) to your own account. This is a great way to get started quickly. If you’d like to use a theme, fork the repo and name it username.github.io where username is your GitHub username. You can then edit the files to customise your website.How to Fork a repositoryCreate your websiteNow you have a repository, you can start creating your website. You can do this in a number of ways. You can create a website using HTML, Markdown, or Jekyll. If you’re not sure which one to use, I’d recommend starting with Markdown. It’s easy to learn and you can always switch to HTML or Jekyll later if you want to.Get a local copy of your repositoryAt the moment your repository and all the files are stored on GitHub’s servers. You can edit them online directly, but I’d recommend getting a local copy of your repository. This will allow you to edit your files on your computer and then upload them to GitHub when you’re ready.How to Clone a repository locallyStart making changesNow you have a local copy of your repository, you can start making changes. If you’re using Markdown, you can edit your files using any text editor. If you’re using HTML, you can use any HTML editor. If you’re using Jekyll, you can use any text editor or IDE that supports Jekyll. I’d recommend using Visual Studio Code as it has great support for Markdown and Jekyll.You will see a list of files and folders (directories). Fortunately You can ignore most of these.The areas you’re interested in are:  _posts - This is where your blog posts will go. You can create a new file for each post. The file name should be in the format YYYY-MM-DD-title.md where YYYY-MM-DD is the date of your post and title is the title of your post. For example, 2023-10-04-github-pages.md. You can use Markdown or HTML for your posts.  _pages - This is where your pages will go. You can create a new file for each page. The file name should be in the format title.md where title is the title of your page. For example, about.md. You can use Markdown or HTML for your pages.  _tabs - This is where your tabs will go. You can create a new file for each tab. The file name should be in the format title.md where title is the title of your tab. For example, about.md. You can use Markdown or HTML for your tabs.  _config.yml - This is where you can configure your website. You can change the title, description, and other settings here. You can also change the theme here.  assets - This is where you can store images and other files that you want to use on your website. You can create subfolders to organise your files.The home page in the theme above is a list of the posts, so you’ll need to create at least one post to see anything on your website.Commit your changesTry this out by creating a post, once you’ve done that, you can commit your changes. This will save your changes to your local repository. You can then push your changes to GitHub. This will upload your changes to GitHub and make them available on your website.How to Commit and Push changesView your websiteYou can build your website locally by running bundle exec jekyll serve from the command line. This will build your website and make it available at http://localhost:4000. You can then view your website in your browser at that address.You should see the changes you have made. If you don’t, check the output from the command line to see if there are any errors.Installing Google Analytics and AdsenseIf you’re like me and want to know whether people are actually viewing your content, you can use Google Analytics to track visits to your site easily. Simply sign up for an account and then add the tracking identifier (G-XXXXXXXX) to the _config.yml file in your repository (next to id: under google_analytics:).For google adsense, it’s a little more complicated:  Create an adsense account, add your site and copy the code required (It should look something like this &lt;script data-ad-client=\"ca-pub-XXXXX\" async src=\"https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js\"&gt;&lt;/script&gt;).  Customise your adsense configuration to allow ‘auto ads’. This will place ads on your site automatically where they are most likely to be seen.  Create a folder _includes in your repository, then create an empty file head.html. If this already exists you can skip the next step.  Navigate to the theme repo and copy the content of the file head.html into your new file (example)  In that file, before the line that reads &lt;/head&gt; add the code copied from Google Adsense.What next?Now you have a website, you can start adding content. You can create new posts, pages, and tabs. You can also customise the theme and add your own images and files. To deploy the latest changes simply commit and push your changes to GitHub. The in-built github action will take care of the rest.ConclusionThis is a very basic introduction to GitHub Pages. There is a lot more you can do with it, but hopefully this will get you started. If you have any questions, please feel free to ask in the community. I’d also love to see what you create, so please share your website with us!If you’d like to see a video explaining this process, take a look at this video by Techno Tim.TD;LR?Just want to get on with it?Assuming you have jekyll, git and github already configured, you can fork the repo then use these terminal commands to get your own website up and running in your own github pages repository.Sign in to GitHub and browse to Chirpy Starter, click the button Use this template &gt; Create a new repository, and name the new repository username.github.io, where username represents your GitHub username.On your local computer:# Clone the theme repositorygit clone &lt;your new repository url&gt;# Change directory to your repositorycd &lt;your new repository&gt;# Install the dependenciesbundle# Run local serverbundle exec jekyll serve# Navigate to http://localhost:4000 in your web browser"
  },
  
  {
    "title": "Modular Biped Intro & History | YouTube",
    "url": "/posts/modular-biped-intro-video/",
    "categories": "Robotics, Modular Robotics Framework",
    "tags": "modular, robotics, framework, youtube",
    "date": "2023-08-15 00:00:00 +0100",
    





    
    "snippet": "Since the creation of version 3 of the modular robotics framework, this video covers an overview of the project and the history of each version.The project has evolved over the last few years and a...",
    "content": "Since the creation of version 3 of the modular robotics framework, this video covers an overview of the project and the history of each version.The project has evolved over the last few years and a community has begun to grow around it. This led to a number of changes to allow the platform to be more modular, allowing any number of robots to be created using the software with reuse of any relevant hardware components and modules.This video covers the history of the project and the features of the current version as well as decisions made along the way and future plans for the functionality.If you’re interested in joining the community or building this project yourself, click hereWatch on YouTube now:"
  }
  
]

